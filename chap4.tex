\chapter{Conclusion and Outlook}

The classification performance improvement of using MLP models pretrained on fast simulation datasets on the boosted top quark tagging problem is shown. Pretained models outperformed models trained from scratch across all data regimes tested. The performance improvement was greatest when only 2 million \textsc{Geant4} samples were available and declined as the number of samples increased. With 16 million samples available, the performance improvement was marginal. This highlights the potential of pretrained models to be used when the target dataset is scarce or expensive to generate.

Several variations of this experiment are worth studying. The first is a sweep of a wider variety of model architectures. This study focus on the MLP as a simple and generalizable deep learning model. However, architectures such as graph neural networks (GNNs) and transformers have been shown outperform MLPs and achieve state-of-the-art performance on particle physics tasks like top tagging. These architectures have been known to be data-hungry and require a larger amount of training data. Thus a second dimension to explore would be the effects of the absolute and relative sizes of the pretraining and target datasets. Specifically, what is the relation between the performance delta of pretraining and the pretraining dataset size?

Another promising direction for further work would be adding additional datasets to pretraining that are not necessarily simulations of the target process. This would allow the model to build a robust general understanding of jet physics interactions before specializing in the target dataset. Recent work has shown that following the LLM paradigm of building a foundation model for jet physics is viable and can yield state-of-the-art performance \cite{mikuni2024omnilearn}. Such models may be improved by adding additional training data to the pretraining that is close in distribution to the target process.

\section{Code Availability}
The code for this thesis is made public and can be found at \href{https://github.com/evilpegasus/transfer-learning}{https://github.com/evilpegasus/transfer-learning}.

\section{Acknowledgements}

This work was advised by Benjamin Nachman and would not have been possible without his guidance. Computational resources were provided by the Nachman Group in the Physics Division at the Lawrence Berkeley National Lab. The \textsc{Delphes} dataset used in this study was generated by Daniel Whiteson. Kevin Greif assisted in setting up the datasets. This research used resources of the National Energy Research Scientific Computing Center (NERSC), a Department of Energy Office of Science User Facility using NERSC award m3246 for 2022-2024.

This thesis was submitted in partial satisfaction of the requirements for the Electrical Engineering and Computer Science Honors Program in the Department of Electrical Engineering and Computer Science at the University of California, Berkeley.